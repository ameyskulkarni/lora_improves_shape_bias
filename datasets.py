"""Dataloaders for ImageNet variants."""
import os
from pathlib import Path
from typing import Optional, Callable
import torch
from torch.utils.data import DataLoader, Dataset, ConcatDataset
from torchvision import transforms
from torchvision.datasets import ImageFolder
from PIL import Image
from pathlib import Path
from typing import Optional, Callable
from torch.utils.data import Dataset


class ImageNetV2Dataset(Dataset):
    """ImageNet-V2 with correct label mapping.

    ImageNet-V2 folders are named 0-999 (as strings), but ImageFolder
    sorts alphabetically: 0, 1, 10, 100, ... which breaks label mapping.
    """

    VALID_EXTENSIONS = {'.jpg', '.jpeg', '.png', '.JPEG', '.JPG', '.PNG'}

    def __init__(self, root: str, transform: Optional[Callable] = None):
        self.root = Path(root)
        self.transform = transform
        self.samples = []

        # Iterate folders in numeric order
        for class_idx in range(1000):
            class_dir = self.root / str(class_idx)
            if class_dir.exists():
                for img_path in class_dir.iterdir():
                    if img_path.suffix in self.VALID_EXTENSIONS:
                        self.samples.append((str(img_path), class_idx))

        if len(self.samples) == 0:
            raise RuntimeError(
                f"No images found in {root}. "
                f"Expected structure: {root}/0/, {root}/1/, ..., {root}/999/ "
                f"with image files inside each folder."
            )
        print(f"ImageNetV2Dataset: found {len(self.samples)} images")

    def __len__(self):
        return len(self.samples)

    def __getitem__(self, idx):
        path, label = self.samples[idx]
        img = Image.open(path).convert('RGB')
        if self.transform:
            img = self.transform(img)
        return {"pixel_values": img, "labels": label}


class ImageNetDataset(Dataset):
    """Unified dataset for ImageNet variants."""

    VARIANTS = ["imagenet", "stylized_imagenet", "imagenet_c", "cue_conflict", "imagenetv2", "imagenet_sketch"]

    def __init__(
        self,
        root: str,
        variant: str = "imagenet",
        split: str = "train",
        transform: Optional[Callable] = None,
        corruption_type: Optional[str] = None,
        severity: int = 3,
    ):
        self.variant = variant
        self.transform = transform
        self._v2_dataset = None
        self.dataset = None

        # Handle ImageNet-V2 specially due to folder naming
        if variant == "imagenetv2":
            data_path = Path(root) / "imagenet-v2"/"imagenetv2-matched-frequency-val"/ split
            if not data_path.exists():
                raise RuntimeError(
                    f"ImageNet-V2 not found at {data_path}. "
                    f"Download from https://github.com/modestyachts/ImageNetV2 "
                    f"and ensure folder is named 'imagenetv2-matched-frequency-format-val'"
                )
            self._v2_dataset = ImageNetV2Dataset(str(data_path), transform)
            self.classes = [str(i) for i in range(1000)]
            self.class_to_idx = {str(i): i for i in range(1000)}
            return

        # Build path based on variant
        if variant == "imagenet":
            data_path = Path(root) / "imagenet" / split
        elif variant == "stylized_imagenet":
            data_path = Path(root) / "stylized_imagenet" / split
        elif variant == "imagenet_c":
            assert corruption_type, "Specify corruption_type for ImageNet-C"
            data_path = Path(root) / "imagenet-c" / corruption_type / str(severity)
        elif variant == "cue_conflict":
            data_path = Path(root) / "cue-conflict"
        elif variant == "imagenet_sketch":
            data_path = Path(root) / "imagenet-sketch" / "sketch"
        else:
            raise ValueError(f"Unknown variant: {variant}")

        self.dataset = ImageFolder(str(data_path), transform=None)
        self.classes = self.dataset.classes
        self.class_to_idx = self.dataset.class_to_idx

    def __len__(self):
        if self._v2_dataset is not None:
            return len(self._v2_dataset)
        return len(self.dataset)

    def __getitem__(self, idx):
        if self._v2_dataset is not None:
            return self._v2_dataset[idx]

        img, label = self.dataset[idx]
        if self.transform:
            img = self.transform(img)
        return {"pixel_values": img, "labels": label}


class ImageNetDepthDataset(Dataset):
    """ImageNet depth maps as training data.

    Loads pseudo-depth maps generated by DepthAnything and treats them
    as individual training samples (repeated 3x for RGB compatibility).

    Key insight: Depth maps contain ZERO texture information - only 3D
    geometric structure. Training on depth forces shape-based learning.

    Expected structure:
        {data_root}/imagenet_depth/{split}/{class_id}/*.png
    """

    VALID_EXTENSIONS = {'.png', '.PNG', '.jpg', '.jpeg', '.JPEG', '.JPG'}

    def __init__(
            self,
            root: str,
            split: str = "train",
            transform: Optional[Callable] = None,
            depth_normalization: str = "imagenet",  # or "depth_specific"
    ):
        """
        Args:
            root: Path to data directory
            split: "train" or "val"
            transform: Transforms to apply (should match RGB transforms)
            depth_normalization: How to normalize depth values
                - "imagenet": Use ImageNet mean/std (recommended for frozen backbone)
                - "depth_specific": Use depth-specific normalization
        """
        self.root = Path(root)
        self.split = split
        self.transform = transform
        self.depth_normalization = depth_normalization
        self.samples = []

        # Path to depth maps (mirrors ImageNet structure)
        depth_root = self.root / "imagenet_depth" / split

        if not depth_root.exists():
            raise RuntimeError(
                f"Depth maps not found at {depth_root}. "
                f"Run generate_depth.py first to create depth maps."
            )

        # Load samples with same label structure as ImageNet
        class_dirs = sorted([d for d in depth_root.iterdir() if d.is_dir()])

        # Build class_to_idx mapping (same as ImageNet)
        self.classes = [d.name for d in class_dirs]
        self.class_to_idx = {cls: idx for idx, cls in enumerate(self.classes)}

        # Collect all depth map paths
        for class_dir in class_dirs:
            class_idx = self.class_to_idx[class_dir.name]
            for img_path in class_dir.iterdir():
                if img_path.suffix in self.VALID_EXTENSIONS:
                    self.samples.append((str(img_path), class_idx))

        if len(self.samples) == 0:
            raise RuntimeError(
                f"No depth maps found in {depth_root}. "
                f"Expected structure: {depth_root}/n01440764/*.png"
            )

        print(f"ImageNetDepthDataset: found {len(self.samples)} depth maps")

    def __len__(self):
        return len(self.samples)

    def __getitem__(self, idx):
        path, label = self.samples[idx]

        # Load depth map as grayscale
        depth = Image.open(path).convert('L')  # Single channel

        # Convert to RGB by repeating 3x
        # This maintains compatibility with ViT's 3-channel input
        depth_rgb = Image.merge('RGB', (depth, depth, depth))

        # Apply same transforms as RGB images
        if self.transform:
            depth_rgb = self.transform(depth_rgb)

        return {"pixel_values": depth_rgb, "labels": label}


class ShapeStimuliDataset(Dataset):
    """Dataset for shape-stimuli from model-vs-human repository.

    Handles two different folder structures and label types:
    1. Edge and silhouette: category-based folders, 16-class labels (0-15)
    2. Other datasets: dnn/session-1 folders, ImageNet-1000 labels

    Supports datasets: edge, silhouette, color, contrast, high-pass, low-pass,
    phase-scrambling, power-equalization, false-color, rotation, eidolonI,
    eidolonII, eidolonIII, uniform-noise, sketch.
    """

    # Mapping of dataset folder names to consistent names
    DATASET_NAMES = {
        'colour': 'color',
        'contrast': 'contrast',
        'edge': 'edge',
        'false-colour': 'false-color',
        'high-pass': 'high-pass',
        'low-pass': 'low-pass',
        'phase-scrambling': 'phase-scrambling',
        'power-equalisation': 'power-equalization',
        'rotation': 'rotation',
        'silhouette': 'silhouette',
        'sketch': 'sketch',
        'eidolonI': 'eidolonI',
        'eidolonII': 'eidolonII',
        'eidolonIII': 'eidolonIII',
        'uniform-noise': 'uniform-noise',
        'stylized': 'stylized',
    }

    # Datasets with category-based folder structure
    CATEGORY_FOLDER_DATASETS = {'edge', 'silhouette'}

    # ImageNet-16 categories (used for dnn/session-1 datasets)
    IMAGENET_16_CATEGORIES = {
        'airplane': 404, 'bear': 294, 'bicycle': 671, 'bird': 14,
        'boat': 472, 'bottle': 440, 'car': 817, 'cat': 282,
        'chair': 559, 'clock': 409, 'dog': 254, 'elephant': 386,
        'keyboard': 508, 'knife': 499, 'oven': 766, 'truck': 867,
    }

    # 16-class category indices (for edge and silhouette)
    CATEGORY_TO_IDX_16CLASS = {
        'airplane': 0, 'bear': 1, 'bicycle': 2, 'bird': 3,
        'boat': 4, 'bottle': 5, 'car': 6, 'cat': 7,
        'chair': 8, 'clock': 9, 'dog': 10, 'elephant': 11,
        'keyboard': 12, 'knife': 13, 'oven': 14, 'truck': 15
    }

    def __init__(
            self,
            root: str,
            dataset_name: str,
            transform: Optional[Callable] = None,
    ):
        """
        Args:
            root: Path to shape-stimuli folder
            dataset_name: Name of the dataset (e.g., 'edge', 'silhouette', 'color')
            transform: Transform to apply to images
        """
        self.transform = transform
        self.dataset_name = dataset_name
        self.samples = []

        # Datasets that use 16-class aggregation (labels are 0-15)
        self.use_16_class = dataset_name in {'edge', 'silhouette'}

        # Find the actual folder name (handle naming variations)
        root_path = Path(root)
        dataset_folder = None

        for folder, name in self.DATASET_NAMES.items():
            if name == dataset_name:
                candidate = root_path / folder
                if candidate.exists():
                    dataset_folder = candidate
                    break

        if dataset_folder is None:
            raise RuntimeError(
                f"Dataset '{dataset_name}' not found in {root}. "
                f"Expected folder: {root}/{dataset_name}/"
            )

        # Handle different folder structures
        if dataset_name in self.CATEGORY_FOLDER_DATASETS:
            # Structure: dataset_name/{category}/{category}N.png
            self._load_category_folders(dataset_folder)
        else:
            # Structure: dataset_name/dnn/session-1/*.png
            self._load_dnn_session(dataset_folder)

        if len(self.samples) == 0:
            raise RuntimeError(
                f"No valid images found in {dataset_folder}. "
                f"Check folder structure for {dataset_name}."
            )

        print(f"ShapeStimuliDataset ({dataset_name}): found {len(self.samples)} images")
        if self.use_16_class:
            print(f"  Using 16-class labels (0-15) with aggregation")

    def _load_category_folders(self, dataset_folder: Path):
        """Load images from category-based folder structure (edge, silhouette).

        Structure: dataset_name/{category}/{category}N.png
        Example: edge/airplane/airplane1.png

        For edge/silhouette: labels are 0-15 (for 16-class aggregation)
        """
        for category in self.IMAGENET_16_CATEGORIES.keys():
            category_dir = dataset_folder / category
            if category_dir.exists():
                # Find all images for this category
                for img_path in sorted(category_dir.glob(f"{category}*.png")):
                    if self.use_16_class:
                        # For edge/silhouette: use 16-class index (0-15)
                        label = self.CATEGORY_TO_IDX_16CLASS[category]
                    else:
                        # Shouldn't happen, but fallback to ImageNet ID
                        label = self.IMAGENET_16_CATEGORIES[category]

                    self.samples.append((str(img_path), label))

    def _load_dnn_session(self, dataset_folder: Path):
        """Load images from dnn/session-1 folder structure.

        Structure: dataset_name/dnn/session-1/*.png
        Filename format: XXXX_<prefix>_<category>_<suffix>.png

        Labels are ImageNet class IDs (0-999)
        """
        session_path = dataset_folder / "dnn" / "session-1"

        if not session_path.exists():
            raise RuntimeError(
                f"Expected folder not found: {session_path}. "
                f"For most datasets, structure should be: dataset_name/dnn/session-1/"
            )

        # Collect all image files
        for img_path in sorted(session_path.glob("*.png")):
            # Extract category from filename
            # Format: XXXX_<prefix>_<category>_<suffix>.png
            # e.g., 0001_cl_dnn_cr_oven_40_n04111531_14126.png
            parts = img_path.stem.split('_')
            category = None

            # Find the category in the filename
            for part in parts:
                if part in self.IMAGENET_16_CATEGORIES:
                    category = part
                    break

            if category is not None:
                # FIX: Use appropriate label based on evaluation method
                label = self.CATEGORY_TO_IDX_16CLASS[category]

                self.samples.append((str(img_path), label))

    def __len__(self):
        return len(self.samples)

    def __getitem__(self, idx):
        path, label = self.samples[idx]
        img = Image.open(path).convert('RGB')
        if self.transform:
            img = self.transform(img)
        return {"pixel_values": img, "labels": label}


def get_transforms(processor, is_train: bool = True):
    """Get transforms compatible with ViT/DeiT processor.

    Uses BICUBIC interpolation and correct ImageNet normalization.
    Note: HuggingFace DeiT processor has wrong mean/std ([0.5,0.5,0.5]),
    so we override with correct ImageNet values.
    """
    # DeiT uses ImageNet normalization, NOT [0.5, 0.5, 0.5]
    # HuggingFace processor has wrong values, so we hardcode correct ones
    IMAGENET_MEAN = [0.485, 0.456, 0.406]
    IMAGENET_STD = [0.229, 0.224, 0.225]

    normalize = transforms.Normalize(mean=IMAGENET_MEAN, std=IMAGENET_STD)

    # Handle different processor.size formats
    if isinstance(processor.size, dict):
        if "height" in processor.size:
            size = processor.size["height"]
        elif "shortest_edge" in processor.size:
            size = processor.size["shortest_edge"]
        else:
            size = 224
    elif isinstance(processor.size, int):
        size = processor.size
    else:
        size = 224

    # DeiT uses crop_pct=0.875, so eval resize = 224/0.875 = 256
    eval_resize = int(size / 0.875)  # 256 for size=224

    if is_train:
        return transforms.Compose([
            transforms.RandomResizedCrop(
                size
            ),
            transforms.RandomHorizontalFlip(),
            transforms.ToTensor(),
            normalize,
        ])
    return transforms.Compose([
        transforms.Resize(eval_resize),
        transforms.CenterCrop(size),
        transforms.ToTensor(),
        normalize,
    ])


def create_dataloaders_mixed(
    data_root: str,
    processor,
    train_variant: str = "stylized_imagenet",
    val_variant: str = "imagenet",
    batch_size: int = 64,
    num_workers: int = 4,
    generator: torch.Generator = None,
    worker_init_fn = None,
) -> tuple[DataLoader, DataLoader]:
    """Create train/val dataloaders with different variants.

    Supported variant options:
    - "imagenet": Standard ImageNet only
    - "stylized_imagenet": Stylized ImageNet only
    - "depth": Depth maps only (for ablation)
    - "mixed_stylized": ImageNet + Stylized ImageNet
    - "mixed_depth": ImageNet + Depth maps
    - "mixed_all": ImageNet + Stylized + Depth
    """
    train_transform = get_transforms(processor, is_train=True)
    val_transform = get_transforms(processor, is_train=False)

    # Handle mixed training (ImageNet + Stylized)
    if train_variant == "mixed_stylized":
        print("Creating mixed training dataset (ImageNet + Stylized)")
        imagenet_ds = ImageNetDataset(
            data_root, variant="imagenet", split="train",
            transform=train_transform,
        )
        stylized_ds = ImageNetDataset(
            data_root, variant="stylized_imagenet", split="train",
            transform=train_transform,
        )
        train_ds = ConcatDataset([imagenet_ds, stylized_ds])
        print(f"  ImageNet: {len(imagenet_ds)} images")
        print(f"  Stylized: {len(stylized_ds)} images")
        print(f"  Total: {len(train_ds)} images")
    elif train_variant == "mixed_depth":
        print("Creating mixed training dataset (ImageNet + Depth)")
        imagenet_ds = ImageNetDataset(
            data_root, variant="imagenet", split="train",
            transform=train_transform,
        )
        depth_ds = ImageNetDepthDataset(
            data_root, split="train", transform=train_transform,
        )
        train_ds = ConcatDataset([imagenet_ds, depth_ds])
        print(f"  ImageNet: {len(imagenet_ds)} images")
        print(f"  Depth: {len(depth_ds)} images")
        print(f"  Total: {len(train_ds)} images")
    elif train_variant == "mixed_all":
        # NEW: ImageNet + Stylized + Depth (triple mix)
        print("Creating mixed training dataset (ImageNet + Stylized + Depth)")
        imagenet_ds = ImageNetDataset(
            data_root, variant="imagenet", split="train",
            transform=train_transform,
        )
        stylized_ds = ImageNetDataset(
            data_root, variant="stylized_imagenet", split="train",
            transform=train_transform,
        )
        depth_ds = ImageNetDepthDataset(
            data_root, split="train", transform=train_transform,
        )
        train_ds = ConcatDataset([imagenet_ds, stylized_ds, depth_ds])
        print(f"  ImageNet: {len(imagenet_ds)} images")
        print(f"  Stylized: {len(stylized_ds)} images")
        print(f"  Depth: {len(depth_ds)} images")
        print(f"  Total: {len(train_ds)} images")
    elif train_variant == "depth":
        print("Creating training dataset = Depth")
        train_ds = ImageNetDepthDataset(
            data_root, split="train", transform=train_transform,
        )
    else:
        train_ds = ImageNetDataset(
            data_root, variant=train_variant, split="train",
            transform=train_transform,
        )
        print(f"Training dataset: {train_variant} ({len(train_ds)} images)")



    if val_variant == "mixed_stylized":
        print("Creating mixed val dataset (ImageNet + Stylized)")
        val_imagenet_ds = ImageNetDataset(
            data_root, variant="imagenet", split="val",
            transform=val_transform,
        )
        val_stylized_ds = ImageNetDataset(
            data_root, variant="stylized_imagenet", split="val",
            transform=val_transform,
        )
        val_ds = ConcatDataset([val_imagenet_ds, val_stylized_ds])
        print(f"  ImageNet: {len(val_imagenet_ds)} images")
        print(f"  Stylized: {len(val_stylized_ds)} images")
        print(f"  Total: {len(val_ds)} images")

    elif val_variant == "mixed_depth":
        print("Creating val dataset (ImageNet + Depth)")
        val_imagenet_ds = ImageNetDataset(
            data_root, variant="imagenet", split="val",
            transform=val_transform,
        )
        val_depth_ds = ImageNetDepthDataset(
            data_root, split="val", transform=val_transform,
        )
        val_ds = ConcatDataset([val_imagenet_ds, val_depth_ds])
        print(f"  ImageNet: {len(val_imagenet_ds)} images")
        print(f"  Depth: {len(val_depth_ds)} images")
        print(f"  Total: {len(val_ds)} images")
    elif val_variant == "mixed_all":
        print("Creating val dataset (ImageNet + Stylized + Depth)")
        val_imagenet_ds = ImageNetDataset(
            data_root, variant="imagenet", split="val",
            transform=val_transform,
        )
        val_stylized_ds = ImageNetDataset(
            data_root, variant="stylized_imagenet", split="val",
            transform=val_transform,
        )
        val_depth_ds = ImageNetDepthDataset(
            data_root, split="val", transform=val_transform,
        )
        val_ds = ConcatDataset([val_imagenet_ds, val_stylized_ds, val_depth_ds])
        print(f"  ImageNet: {len(val_imagenet_ds)} images")
        print(f"  Stylized: {len(val_stylized_ds)} images")
        print(f"  Depth: {len(val_depth_ds)} images")
        print(f"  Total: {len(val_ds)} images")
    elif val_variant == "depth":
        print("Creating val dataset = Depth")
        val_ds = ImageNetDepthDataset(
            data_root, split="val", transform=val_transform,
        )
        print(f"  Total: {len(val_ds)} images")
    else:
        val_ds = ImageNetDataset(
            data_root, variant=val_variant, split="val",
            transform=val_transform,
        )
        print(f"Val dataset: {val_variant} ({len(val_ds)} images)")


    train_loader = DataLoader(
        train_ds, batch_size=batch_size, shuffle=True,
        num_workers=num_workers, pin_memory=True, drop_last=True, generator=generator,
        worker_init_fn=worker_init_fn,
    )
    val_loader = DataLoader(
        val_ds, batch_size=batch_size, shuffle=False,
        num_workers=num_workers, pin_memory=True, generator=generator,
        worker_init_fn=worker_init_fn,
    )
    return train_loader, val_loader


# ==================== HELPER FUNCTIONS ====================

def verify_depth_dataset(data_root: str, split: str = "val"):
    """Verify depth dataset is properly set up and aligned with ImageNet.

    Usage:
        python -c "from datasets import verify_depth_dataset; verify_depth_dataset('/path/to/data')"
    """
    from pathlib import Path

    imagenet_root = Path(data_root) / "imagenet" / split
    depth_root = Path(data_root) / "imagenet_depth" / split

    print(f"Verifying depth dataset alignment...")
    print(f"  ImageNet: {imagenet_root}")
    print(f"  Depth: {depth_root}")

    if not imagenet_root.exists():
        print(f"  ERROR: ImageNet not found at {imagenet_root}")
        return False

    if not depth_root.exists():
        print(f"  ERROR: Depth maps not found at {depth_root}")
        return False

    # Check class alignment
    imagenet_classes = set(d.name for d in imagenet_root.iterdir() if d.is_dir())
    depth_classes = set(d.name for d in depth_root.iterdir() if d.is_dir())

    missing_in_depth = imagenet_classes - depth_classes
    extra_in_depth = depth_classes - imagenet_classes

    if missing_in_depth:
        print(f"  WARNING: {len(missing_in_depth)} classes missing in depth: {list(missing_in_depth)[:5]}...")
    if extra_in_depth:
        print(f"  WARNING: {len(extra_in_depth)} extra classes in depth: {list(extra_in_depth)[:5]}...")

    # Count images
    imagenet_count = sum(
        len(list((imagenet_root / c).glob("*")))
        for c in imagenet_classes
    )
    depth_count = sum(
        len(list((depth_root / c).glob("*.png")))
        for c in depth_classes
    )

    print(f"  ImageNet images: {imagenet_count}")
    print(f"  Depth images: {depth_count}")
    print(f"  Coverage: {depth_count / imagenet_count * 100:.1f}%")

    return len(missing_in_depth) == 0 and depth_count > 0